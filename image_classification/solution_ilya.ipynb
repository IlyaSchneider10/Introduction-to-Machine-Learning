{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2054b8a9990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "max_cpus = mp.cpu_count()\n",
    "batch_size = 64 #consider going up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean and sd values for each of the channel of the images to be able to use them for the actual transformation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor with values between 0 and 1\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "my_dataset = datasets.ImageFolder(root=\"dataset/\", transform=transform)\n",
    "\n",
    "# Compute the mean and standard deviation of pixel values for each channel separately\n",
    "mean = np.mean([np.mean(x.numpy(), axis=(1,2)) for x, _ in my_dataset], axis=0)\n",
    "std = np.mean([np.std(x.numpy(), axis=(1,2)) for x, _ in my_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings():\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "    # TODO: define a transform to pre-process the images\n",
    "    train_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip() ,transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "    # run out of memory\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size= batch_size,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=max_cpus)\n",
    "\n",
    "    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "    #  more info here: https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "    model = models.resnet50(pretrained = True)\n",
    "\n",
    "    # # Remove last layer to obtain embeddings\n",
    "    model_embeddings = nn.Sequential(*list(model.children())[:-1])\n",
    "    model_embeddings = model_embeddings.to(device)\n",
    "\n",
    "    embedding_size = 2048 # Dummy variable, replace with the actual embedding size once you \n",
    "    # pick your model\n",
    "    num_images = len(train_dataset)\n",
    "    embeddings = np.zeros((num_images, embedding_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            bacth_embeddings = model_embeddings(data[0])\n",
    "            bacth_embeddings = bacth_embeddings.view(bacth_embeddings.size(0), -1)\n",
    "            bacth_embeddings = bacth_embeddings.to(device)\n",
    "            start_index = i * train_loader.batch_size\n",
    "            end_index = start_index + bacth_embeddings.size(0)\n",
    "            embeddings[start_index:end_index] = bacth_embeddings.cpu().numpy()\n",
    "\n",
    "            del bacth_embeddings\n",
    "\n",
    "    # # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "    # # model to access the embeddings the model generates. \n",
    "\n",
    "    np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = generate_embeddings()\n",
    "# train_features, train_labels = next(iter(train_loader))\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img[0], cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained = True)\n",
    "\n",
    "# # Remove last layer to obtain embeddings\n",
    "# model_embeddings = nn.Sequential(*list(model.children())[:-1])\n",
    "# model_embeddings = model_embeddings.to(device)\n",
    "\n",
    "# a = model_embeddings(train_features)\n",
    "# aa = a.view(a.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schni\\OneDrive\\Documents\\Switzerland\\Bottmedical\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('dataset/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    l2_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    norm_emb = embeddings/l2_norm\n",
    "    return norm_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                         transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    # TODO: Normalize the embeddings across the dataset\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i].replace('food\\\\', '')] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory\n",
    "def create_loader_from_np(X, y = None, train = True, batch_size = batch_size, shuffle=True, num_workers = max_cpus):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a model. Here, the basic structure is defined, but you need to fill in the details\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048*3, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    n_epochs = 20\n",
    "\n",
    "    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "    # of the training data as a validation split. After each epoch, compute the loss on the \n",
    "    # validation split and print it out. This enables you to see how your model is performing \n",
    "    # on the validation data before submitting the results on the server. After choosing the \n",
    "    # best model, train it on the whole training data.\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    X_train, y_train = train_loader.dataset.tensors\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train.numpy(), y_train.numpy(), test_size=0.20, random_state = 1)\n",
    "\n",
    "    train_loader = create_loader_from_np(X_train, y_train, train=True, batch_size = batch_size)\n",
    "    val_loader = create_loader_from_np(X_val, y_val, train=True, batch_size = batch_size)\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        for [X, y] in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X).squeeze()\n",
    "            loss = loss_function(outputs, y.type(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for [X_val, y_val] in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs_val = model(X_val).squeeze()\n",
    "                loss_val = loss_function(outputs_val, y_val.type(torch.float))\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Switch back to train mode\n",
    "        model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch= x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"results_2.txt\", predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "X_test, _ = get_data(TEST_TRIPLETS, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader_from_np(X, y, train = True, batch_size =  batch_size)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Validation Loss: 129.1953\n",
      "Epoch 2/20, Validation Loss: 128.4733\n",
      "Epoch 3/20, Validation Loss: 128.1851\n",
      "Epoch 4/20, Validation Loss: 127.7206\n",
      "Epoch 5/20, Validation Loss: 127.8271\n",
      "Epoch 6/20, Validation Loss: 126.7614\n",
      "Epoch 7/20, Validation Loss: 127.0026\n",
      "Epoch 8/20, Validation Loss: 126.5300\n",
      "Epoch 9/20, Validation Loss: 125.8055\n",
      "Epoch 10/20, Validation Loss: 125.8481\n",
      "Epoch 11/20, Validation Loss: 125.2706\n",
      "Epoch 12/20, Validation Loss: 125.9160\n",
      "Epoch 13/20, Validation Loss: 125.0563\n",
      "Epoch 14/20, Validation Loss: 126.0779\n",
      "Epoch 15/20, Validation Loss: 125.3078\n",
      "Epoch 16/20, Validation Loss: 130.0714\n",
      "Epoch 17/20, Validation Loss: 130.5260\n",
      "Epoch 18/20, Validation Loss: 134.4161\n",
      "Epoch 19/20, Validation Loss: 126.5535\n",
      "Epoch 20/20, Validation Loss: 127.5071\n"
     ]
    }
   ],
   "source": [
    "model = train_model(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test data\n",
    "test_model(model, test_loader)\n",
    "print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNet\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    The model class, which defines our classifier.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048*3, 3072)\n",
    "        self.fc2 = nn.Linear(3072, 1536)\n",
    "        self.fc3 = nn.Linear(1536, 768)\n",
    "        self.fc4 = nn.Linear(768, 384)\n",
    "        self.fc5 = nn.Linear(384, 192)\n",
    "        self.fc6 = nn.Linear(192, 96)\n",
    "        self.fc7 = nn.Linear(96, 48)\n",
    "        self.fc8 = nn.Linear(48, 24)\n",
    "        self.fc9 = nn.Linear(24, 12)\n",
    "        self.fc10 = nn.Linear(12, 6)\n",
    "        self.fc11 = nn.Linear(6, 3)\n",
    "        self.fc11 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc7(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc8(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc9(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc10(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc11(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def train_model(train_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    n_epochs = 10\n",
    "\n",
    "    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "    # of the training data as a validation split. After each epoch, compute the loss on the \n",
    "    # validation split and print it out. This enables you to see how your model is performing \n",
    "    # on the validation data before submitting the results on the server. After choosing the \n",
    "    # best model, train it on the whole training data.\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    X_train, y_train = train_loader.dataset.tensors\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train.numpy(), y_train.numpy(), test_size=0.20, random_state = 1)\n",
    "\n",
    "    train_loader = create_loader_from_np(X_train, y_train, train=True, batch_size = batch_size)\n",
    "    val_loader = create_loader_from_np(X_val, y_val, train=True, batch_size = batch_size)\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        for [X, y] in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X).squeeze()\n",
    "            loss = loss_function(outputs, y.type(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for [X_val, y_val] in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs_val = model(X_val).squeeze()\n",
    "                loss_val = loss_function(outputs_val, y_val.type(torch.float))\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Switch back to train mode\n",
    "        model.train()\n",
    "    return model\n",
    "\n",
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch= x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"results_4.txt\", predictions, fmt='%i')\n",
    "\n",
    "model = train_model(train_loader)\n",
    "test_model(model, test_loader)\n",
    "print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a model. Here, the basic structure is defined, but you need to fill in the details\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048*3, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_model(train_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    n_epochs = 50\n",
    "\n",
    "    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "    # of the training data as a validation split. After each epoch, compute the loss on the \n",
    "    # validation split and print it out. This enables you to see how your model is performing \n",
    "    # on the validation data before submitting the results on the server. After choosing the \n",
    "    # best model, train it on the whole training data.\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    X_train, y_train = train_loader.dataset.tensors\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train.numpy(), y_train.numpy(), test_size=0.20, random_state = 1)\n",
    "\n",
    "    train_loader = create_loader_from_np(X_train, y_train, train=True, batch_size = batch_size)\n",
    "    val_loader = create_loader_from_np(X_val, y_val, train=True, batch_size = batch_size)\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        for [X, y] in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X).squeeze()\n",
    "            loss = loss_function(outputs, y.type(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for [X_val, y_val] in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs_val = model(X_val).squeeze()\n",
    "                loss_val = loss_function(outputs_val, y_val.type(torch.float))\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Switch back to train mode\n",
    "        model.train()\n",
    "    return model\n",
    "\n",
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch= x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"results_3.txt\", predictions, fmt='%i')\n",
    "\n",
    "model = train_model(train_loader)\n",
    "test_model(model, test_loader)\n",
    "print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "# generate embedding for each image in the dataset\n",
    "# if(os.path.exists('dataset/embeddings.npy') == False):\n",
    "#     generate_embeddings()\n",
    "\n",
    "# load the training and testing data\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "X_test, _ = get_data(TEST_TRIPLETS, train=False)\n",
    "\n",
    "# Create data loaders for the training and testing data\n",
    "train_loader = create_loader_from_np(X, y, train = True, batch_size=64)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "\n",
    "# define a model and train it\n",
    "model = train_model(train_loader)\n",
    "\n",
    "# test the model on the test data\n",
    "test_model(model, test_loader)\n",
    "print(\"Results saved to results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
