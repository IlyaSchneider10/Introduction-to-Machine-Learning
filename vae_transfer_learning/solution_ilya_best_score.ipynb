{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "max_cpus = mp.cpu_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"public/pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"public/pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"public/train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"public/train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"public/test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale Lumo and gap labels\n",
    "\n",
    "# scaler_pretrain = StandardScaler()\n",
    "# y_pretrain_scaled = scaler_pretrain.fit_transform(y_pretrain.reshape(-1, 1)).flatten()\n",
    "\n",
    "# scaler_train = StandardScaler()\n",
    "# y_train_scaled = scaler_train.fit_transform(y_train.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 750),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(750, latent_dim),\n",
    "        )\n",
    "\n",
    "# 1000 -> 750 -> 500 achieves 0.0483 reconstruction error after 30 epochs\n",
    "# 0.0442 after 40 epochs\n",
    "# 0.0569 after 20 epochs\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 750),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(750, input_dim),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        predicted = self.predictor(encoded)\n",
    "        \n",
    "        return encoded, decoded, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(y_pred,y):\n",
    "    return torch.sqrt(torch.mean((y_pred-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "\n",
    "    # Pretraining data loading\n",
    "    x_tr, x_val, y_tr, y_val  = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val, y_tr, y_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float), torch.tensor(y_tr, dtype=torch.float),  torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    train_dataset = TensorDataset(x_tr, y_tr)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # model declaration\n",
    "    model = AutoEncoder(1000, 32)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "\n",
    "    n_epochs = 20 # probably will not make big difference, butconsider may be a bit more to reduce reconstruction error\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        for [X, Y] in train_dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            _, reconstruct, prediction = model(X)\n",
    "\n",
    "            loss_1 = RMSELoss(reconstruct, X)\n",
    "            loss_2 = RMSELoss(prediction, Y) \n",
    "            loss = loss_1 + loss_2\n",
    "\n",
    "            # loss_1.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss_1 = 0.0\n",
    "        val_loss_2 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for [X_val, Y_val] in val_dataloader:\n",
    "                X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "                _, reconstruct_val, prediction_val = model(X_val)\n",
    "            \n",
    "                loss_val_1 = RMSELoss(reconstruct_val, X_val)\n",
    "                loss_val_2 = RMSELoss(prediction_val, Y_val)\n",
    "\n",
    "                val_loss_1 += loss_val_1.item()\n",
    "                val_loss_2 += loss_val_2.item()\n",
    "\n",
    "        val_loss_1 /= len(val_dataloader)\n",
    "        val_loss_2 /= len(val_dataloader)\n",
    "        val_loss = val_loss_1 + val_loss_2\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss Reconstruction: {val_loss_1:.4f}, Validation Loss Prediction: {val_loss_2:.4f}, Validation Loss Total: {val_loss:.4f}\")\n",
    "\n",
    "        # Switch back to train mode\n",
    "        model.train()\n",
    "    \n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            features, _, _ = model(x)\n",
    "            features = features.numpy() \n",
    "        return features, model\n",
    "\n",
    "    return make_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Validation Loss Reconstruction: 0.1596, Validation Loss Prediction: 0.4130, Validation Loss Total: 0.5726\n",
      "Epoch 2/20, Validation Loss Reconstruction: 0.1330, Validation Loss Prediction: 0.3937, Validation Loss Total: 0.5267\n",
      "Epoch 3/20, Validation Loss Reconstruction: 0.1237, Validation Loss Prediction: 0.3950, Validation Loss Total: 0.5187\n",
      "Epoch 4/20, Validation Loss Reconstruction: 0.1180, Validation Loss Prediction: 0.3887, Validation Loss Total: 0.5067\n",
      "Epoch 5/20, Validation Loss Reconstruction: 0.1131, Validation Loss Prediction: 0.3876, Validation Loss Total: 0.5008\n",
      "Epoch 6/20, Validation Loss Reconstruction: 0.1097, Validation Loss Prediction: 0.3901, Validation Loss Total: 0.4998\n",
      "Epoch 7/20, Validation Loss Reconstruction: 0.1069, Validation Loss Prediction: 0.3911, Validation Loss Total: 0.4980\n",
      "Epoch 8/20, Validation Loss Reconstruction: 0.1048, Validation Loss Prediction: 0.3928, Validation Loss Total: 0.4976\n",
      "Epoch 9/20, Validation Loss Reconstruction: 0.1027, Validation Loss Prediction: 0.3886, Validation Loss Total: 0.4913\n",
      "Epoch 10/20, Validation Loss Reconstruction: 0.1011, Validation Loss Prediction: 0.3902, Validation Loss Total: 0.4914\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = make_feature_extractor(x_pretrain, y_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = torch.tensor(x_train, dtype=torch.float)\n",
    "x_ptr = torch.tensor(x_pretrain, dtype=torch.float)\n",
    "x_tst = torch.tensor(x_test.to_numpy(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression on pretrain\n",
    "# pretrained_features, _ = feature_extractor(x_ptr)\n",
    "# pretrain_ridge = Ridge()\n",
    "# pretrain_ridge.fit(pretrained_features, y_pretrain_scaled)\n",
    "# pretrain_weights = pretrain_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3062379651720999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Trying to predict the train data using ridge trained on pretrain just to get an estimate\n",
    "\n",
    "# trained_features, _ = feature_extractor(x_tr)\n",
    "# y_train_predicted = pretrain_ridge.predict(trained_features)\n",
    "# y_train_predicted = scaler_train.inverse_transform(y_train_predicted.reshape(-1, 1)).flatten()\n",
    "\n",
    "# np.sqrt(np.mean((y_train - y_train_predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train linear regression on train using weights from pretrain\n",
    "trained_features, _ = feature_extractor(x_tr)\n",
    "train_ridge = Ridge()\n",
    "# train_ridge.coef_ = pretrain_weights\n",
    "train_ridge.fit(trained_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save\n",
    "test_features, _ = feature_extractor(x_tst)\n",
    "y_pred = train_ridge.predict(test_features)\n",
    "# y_pred_original = scaler_train.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(\"results25.csv\", index_label=\"Id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
